{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FPF Gain approximation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sympy import *\n",
    "# import sympy as sp\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "from scipy.stats import norm\n",
    "import scipy.integrate as integrate\n",
    "import math\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "import parameters\n",
    "\n",
    "rc('text',usetex = True)\n",
    "rc('font', **parameters.font)\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining some functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_samples() - Function to generate samples from a multi-dimensional 1d- Gaussian mixture $\\times$  (d-1) independent Gaussian distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(N, mu, sigma_b, w, dim, gm = 1, sigma = None, seed = None):\n",
    "    np.random.seed(seed)\n",
    "    Xi  = np.zeros((N,dim))\n",
    "    for i in range(N):\n",
    "        for d in range(gm):\n",
    "            if np.random.uniform() <= w[0]:\n",
    "                Xi[i,d] = mu[0]  + sigma_b[0] * np.random.normal()\n",
    "            else:\n",
    "                Xi[i,d]  = mu[1]  + sigma_b[1] * np.random.normal()\n",
    "        for d in range(gm, dim):\n",
    "            Xi[i,d] = sigma * np.random.normal()\n",
    "    return Xi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean_squared_error() - Function to compute the mean square error in gain function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(K_exact, K_approx):\n",
    "    N = len(K_exact)\n",
    "    mse = (1/N) * np.linalg.norm(K_exact - K_approx)**2\n",
    "    # mse2 = np.sum(((K_exact - K_approx)**2) *np.concatenate(p_vec(Xi)))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different gain approximation algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gain_rkhs_2N() - Function to approximate FPF gain using optimal RKHS method  \n",
    "uses the extended representer theorem in - https://www.sciencedirect.com/science/article/pii/S0377042707004657?via%3Dihub\n",
    "\n",
    "Algorithm for a scalar example\n",
    "\\begin{equation}\n",
    "\\text{K}(x) = \\sum_{i=1}^N \\Bigl[ \\beta^0_i K(x^i,x) + \\beta^1_i \\frac{\\partial K} {\\partial x}(x^i,x) \\Bigr]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\beta ^* = M^{-1} b\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\begin{aligned}\n",
    "\t\\text{where,\n",
    "\t}\n",
    "\t\\quad\n",
    "\tM & := \\frac{1}{N} \\left[\\begin{array}{c} M_y\\\\ \\hline M_{xy} \\end{array}\\right] [ M_x \\,| M_{xy}] + \\lambda  \\left[\n",
    "\t\\begin{array}{c|c}\n",
    "\tM_0 & M_y \\\\\n",
    "\t\\hline\n",
    "\tM_x & M_{xy}\n",
    "\t\\end{array}\n",
    "\t\\right] \\\\\n",
    "\tb & :=  \\frac{1}{N} \\left[ \\begin{array}{c} M_0 \\\\ \\hline M_x \\end{array}\\right] \\tilde{c}\t\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{aligned}\n",
    "M_0(i,j) &:= K(x^i,x^j)\n",
    "\\quad\n",
    "& M_x(i,j) &:= \\frac{\\partial K}{\\partial x}(x^i,x^j)\n",
    "\\\\\n",
    "M_y(i,j) &:= \\frac{\\partial K}{\\partial y}(x^i,x^j)\n",
    "\\quad\n",
    "& M_{xy}(i,j) &:= \\frac{\\partial^2 K}{\\partial x \\partial y}(x^i,x^j).\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_rkhs_2N(Xi, C, epsilon, Lambda, diag = 0):\n",
    "    start = timer()\n",
    "    N,dim = Xi.shape\n",
    "    K = np.zeros((N,dim))\n",
    "    Ker_x = np.array(np.zeros((N,N,dim)))\n",
    "    Ker_xy = np.array(np.zeros((N,N,dim)))\n",
    "    \n",
    "    Ker = np.exp(- squareform(pdist(Xi,'euclidean'))**2/ (4 * epsilon))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            Ker_x[i,j,:] = -(Xi[i,:]-Xi[j,:]) * Ker[i,j] / (2 * epsilon)\n",
    "            Ker_xy[i,j,:] = -(((Xi[i,:] - Xi[j,:])**2) / (2 * epsilon) -1) * Ker[i,j] / (2 * epsilon) # Negative of the second Gaussian derivative, as this is K_xy and not K_x2\n",
    "    \n",
    "    eta = np.mean(C)\n",
    "    Y = (C -eta)\n",
    "    \n",
    "    # Constructing block matrices for future use\n",
    "    # K_big      = [ Ker Ker_x ; Ker_x' Ker_x_y];\n",
    "    # K_thin_yxy = [ Ker_x ; Ker_x_y]; \n",
    "    # K_thin_x   = [ Ker ; Ker_x'];\n",
    "    K_big      = np.concatenate((np.concatenate((Ker,np.transpose(np.reshape(Ker_x,(N,N)))),axis = 1), np.concatenate((np.reshape(Ker_x,(N,N)), np.reshape(Ker_xy,(N,N))),axis =1)))\n",
    "    K_thin_yxy = np.concatenate((np.transpose(np.reshape(Ker_x,(N,N))), np.reshape(Ker_xy,(N,N))))\n",
    "    # K_thin_xxy = np.concatenate((Ker_x,Ker_xy), axis = 1)\n",
    "    K_thin_x   = np.concatenate((Ker, np.reshape(Ker_x,(N,N))))\n",
    "    \n",
    "    # b used in the extended representer theorem algorithm - searching over all of the Hilbert space H\n",
    "    b_2N        = (1/N) * np.dot(K_thin_x, Y)\n",
    "    M_2N        = Lambda * K_big + (1/N) * np.dot(K_thin_yxy, np.transpose(K_thin_yxy))\n",
    "    if(np.linalg.det(M_2N)!=0):\n",
    "        beta_2N     = np.linalg.solve(M_2N, b_2N) \n",
    "    else:\n",
    "        beta_2N     = np.linalg.lstsq(M_2N,b_2N)[0]\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            K[i,:] = K[i,:] + beta_2N[j] * Ker_x[i,j,:] + beta_2N[N+j] * Ker_xy[i,j,:]\n",
    "            \n",
    "    if diag == 1:\n",
    "        plt.figure()\n",
    "        plt.plot(Xi, Ker[:,0],'r*')\n",
    "        plt.plot(Xi, Ker_x[:,0], 'b*')\n",
    "        plt.plot(Xi, Ker_xy[:,0],'k*')\n",
    "        plt.show()\n",
    "            \n",
    "    end = timer()\n",
    "    print('Time taken for gain_rkhs_2N()' , end - start)\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gain_rkhs_dN() - Extension to d-dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_rkhs_dN(Xi, C, epsilon, Lambda, diag = 1):\n",
    "    start = timer()\n",
    "    \n",
    "    N,dim = Xi.shape\n",
    "    K = np.zeros((N,dim))\n",
    "    Ker_x  = np.array(np.zeros((N,N,dim)))\n",
    "    Ker_xy = np.array(np.zeros((N,N, dim+1, dim+1)))\n",
    "    # Ker_xy = np.array(np.zeros((N,N, dim, dim)))\n",
    "    \n",
    "    K_big  = np.array(np.zeros(((dim+1)*N, (dim+1)*N)))\n",
    "    K_thin_x =np.array(np.zeros(((dim+1)*N, N)))\n",
    "    K_thin_xy = np.array(np.zeros(((dim+1)*N, dim * N)))\n",
    "    \n",
    "    Ker = np.exp(- squareform(pdist(Xi,'euclidean'))**2/ (4 * epsilon))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            Ker_x[i,j,:] = -(Xi[i,:]-Xi[j,:]) * Ker[i,j] / (2 * epsilon)\n",
    "            # Ker_x2[i,j,:] = -(((Xi[i,:] - Xi[j,:])**2)  / (2 * epsilon) -1) * Ker[i,j] / (2 * epsilon) # Negative of the second Gaussian derivative, as this is K_xy and not K_x2\n",
    "    \n",
    "    for d_i in range(dim + 1):\n",
    "        for d_j in range(dim + 1):\n",
    "            if d_i == 0:\n",
    "                if d_j == 0:\n",
    "                    K_big[d_i * N : (d_i + 1) *N, d_j * N : (d_j +1) * N ] = Ker\n",
    "                else:\n",
    "                    K_big[d_i * N : (d_i + 1) *N, d_j * N : (d_j +1) * N ] = np.transpose(Ker_x[:,:,d_j-1])\n",
    "            elif d_j == 0:\n",
    "                K_big[d_i * N : (d_i + 1) *N, d_j * N : (d_j +1) * N ] = Ker_x[:,:,d_i-1]\n",
    "            elif d_i == d_j:\n",
    "                for i in range(N):\n",
    "                    for j in range(N):\n",
    "                        Ker_xy[i,j, d_i, d_j] = -(((Xi[i,d_i-1] - Xi[j,d_i-1])**2)  / (2 * epsilon) -1) * Ker[i,j] / (2 * epsilon) \n",
    "                K_big[d_i * N : (d_i + 1) *N, d_j * N : (d_j +1) * N ] = Ker_xy[:,:,d_i, d_j]\n",
    "            else:\n",
    "                for i in range(N):\n",
    "                    for j in range(N):\n",
    "                        Ker_xy[i,j, d_i, d_j] = -((Xi[i,d_i-1] - Xi[j,d_i-1])* (Xi[i,d_j-1] - Xi[j,d_j-1])) / (2 * epsilon) * Ker[i,j] / (2 * epsilon) # Negative of the second Gaussian derivative, as this is K_xy and not K_x2        \n",
    "                K_big[d_i * N : (d_i + 1) *N, d_j * N : (d_j +1) * N ] = Ker_xy[:,:,d_i, d_j]\n",
    "            \n",
    "    for d_i in range(dim + 1):\n",
    "        if d_i == 0:\n",
    "            K_thin_x[d_i *N :(d_i+1)*N,: ] = Ker\n",
    "        else:\n",
    "            K_thin_x[d_i *N :(d_i+1)*N,: ] = Ker_x[:,:,d_i-1]\n",
    "            \n",
    "    for d_i in range(dim+1):\n",
    "        for d_j in range(dim):\n",
    "            if d_i == 0:\n",
    "                K_thin_xy[d_i * N :(d_i+1)*N, d_j * N : (d_j+1) *N] = np.transpose(Ker_x[:,:,d_j])\n",
    "            else:\n",
    "                K_thin_xy[d_i * N :(d_i+1)*N, d_j * N :(d_j+1) *N] = Ker_xy[:,:,d_i,d_j+1]\n",
    "    \n",
    "    eta = np.mean(C)\n",
    "    Y = (C -eta)\n",
    "    # b used in the extended representer theorem algorithm - searching over all of the Hilbert space H\n",
    "    b_dN        = (1/N) * np.dot(K_thin_x, Y)\n",
    "    M_dN        = Lambda * K_big + (1/N) * np.dot(K_thin_xy, np.transpose(K_thin_xy))\n",
    "    if np.linalg.det(M_dN)!= 0:\n",
    "        beta_dN     = np.linalg.solve(M_dN, b_dN)   \n",
    "    else:\n",
    "        beta_dN     = np.linalg.lstsq(M_dN,b_dN)[0]\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            K[i,:] = K[i,:] + beta_dN[j] * Ker_x[i,j,:] \n",
    "            for d_i in range(dim):\n",
    "                K[i,:] = K[i,:] + beta_dN[(d_i + 1) *N + j] * Ker_xy[i,j,(d_i+1),1:]\n",
    "            \n",
    "    if diag == 1:\n",
    "        plt.figure()\n",
    "        plt.plot(Xi[:,0], Ker[:,1],'r*')\n",
    "        plt.plot(Xi, Ker_x[:,1,0], 'b*')\n",
    "#         plt.plot(Xi, Ker_xy[:,1,1,1],'k*')\n",
    "        plt.show()\n",
    "            \n",
    "    end = timer()\n",
    "    print('Time taken for gain_rkhs_dN()' , end - start)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gain_rkhs_N() - Function to approximate FPF gain using subspace RKHS method  \n",
    "uses normal representer theorem, obtains optimal solution on a subspace of RKHS.\n",
    "\n",
    "Algorithm\n",
    "\\begin{equation}\n",
    "\\text{K}(x) = \\sum_{i=1}^N \\beta^*_i \\frac{\\partial K}{\\partial x} (x^i,x)  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\beta  := M^{-1} b   \\,,\n",
    "\\end{equation}\n",
    "where $ M := N^{-1} M_y M_x + \\lambda M_0$ and $ b := N^{-1} M_0 \\, \\tilde{c}  $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_rkhs_N(Xi, C, epsilon, Lambda, diag = 0):\n",
    "    start = timer()\n",
    "    \n",
    "    N,dim = Xi.shape\n",
    "    K = np.zeros((N,dim))\n",
    "    Ker_x = np.array(np.zeros((N,N,dim)))\n",
    "    Ker_x_sum = np.zeros((N,N))\n",
    "    \n",
    "    Ker = np.exp(- squareform(pdist(Xi,'euclidean'))**2/ (4 * epsilon))    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            Ker_x[i,j,:] = -(Xi[i,:]-Xi[j,:]) * Ker[i,j] / (2 * epsilon)\n",
    "    \n",
    "    eta = np.mean(C)\n",
    "    Y = (C -eta)\n",
    "    \n",
    "    b_N = (1/ N) * np.dot(Ker,Y)\n",
    "    for d in np.arange(dim):\n",
    "        Ker_x_sum+= np.dot(Ker_x[:,:,d], Ker_x[:,:,d].transpose())\n",
    "    M_N = Lambda * Ker + (1/ N) * Ker_x_sum\n",
    "    if(np.linalg.det(M_N)!=0):\n",
    "        beta_N = np.linalg.solve(M_N,b_N)\n",
    "    else:\n",
    "        beta_N = np.linalg.lstsq(M_N,b_N)[0]\n",
    "        \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            K[i,:] = K[i,:] + beta_N[j] * Ker_x[i,j,:]\n",
    "            \n",
    "    if diag == 1:\n",
    "        plt.figure()\n",
    "        plt.plot(Xi, Ker[:,100],'r*')\n",
    "        plt.plot(Xi, Ker_x[:,100,:], 'b*')\n",
    "        plt.show()\n",
    "            \n",
    "    end = timer()\n",
    "    print('Time taken for gain_rkhs_N()' , end - start)\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gain_exact() - Function to compute the exact FPF gain by numerical integration\n",
    "\n",
    "Algorithm\n",
    "\\begin{equation} \n",
    "\\text{K}(x) =  - \\frac{1}{p(x)} \\int_{-\\infty}^{x} (c(y) - \\hat{c}) p(y) dy\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_exact(Xi, c, p):\n",
    "    start = timer()\n",
    "    \n",
    "    N = len(Xi)\n",
    "    K = np.zeros(N)\n",
    "    integral = np.zeros(N)\n",
    "    \n",
    "    step = 0.01\n",
    "    xmax = max(mu) + 10\n",
    "    \n",
    "    p_vec = lambdify(x, p, 'numpy')\n",
    "    c_vec = lambdify(x, c, 'numpy')\n",
    "    cp    = lambdify(x, c*p, 'numpy')\n",
    "    c_hat = integrate.quad(cp, -np.inf, np.inf)[0]\n",
    "    \n",
    "    for i in range(N):\n",
    "        integral[i] = 0\n",
    "        for xj in np.arange(Xi[i], xmax + 10,  step):\n",
    "            integral[i] = integral[i] + p_vec(xj) * ( c_vec(xj) - c_hat) * step\n",
    "        K[i] = integral[i]/ p_vec(Xi[i])\n",
    "            \n",
    "    end = timer()\n",
    "    print('Time taken' , end - start)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scipy.integrate.quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_num_integrate(Xi, c, p, x, d=0):\n",
    "    start = timer()\n",
    "    \n",
    "    N = len(Xi)\n",
    "    K = np.zeros(N)\n",
    "    integral = np.zeros(N)\n",
    "    p_x = lambdify(x[0], p, 'numpy')\n",
    "    cp_x  = lambdify(x[0], c*p, 'numpy')\n",
    "    c_hat = integrate.quad(cp_x, -np.inf, np.inf)[0]\n",
    "    integrand_x = lambdify(x[0], p * (c - c_hat) , 'numpy')\n",
    "    integrand = lambda x: integrand_x(x)\n",
    "   \n",
    "    for i in range(N):\n",
    "        if Xi.shape[1] == 1:\n",
    "            integral[i] = integrate.quad( integrand, -np.inf, Xi[i])[0]\n",
    "            K[i] = - integral[i]/ p_x(Xi[i])\n",
    "        else:\n",
    "            integral[i] = integrate.quad( integrand, -np.inf, Xi[i,d])[0]\n",
    "            K[i] = - integral[i]/ p_x(Xi[i,d])\n",
    "    # K = np.reshape(K,(N,1))\n",
    "    \n",
    "    end = timer()\n",
    "    print('Time taken for gain_num_integrate()' , end - start)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gain_coif() - Function to approximate FPF gain using Markov kernel approx. method -\n",
    "Based on the Markov semigroup approximation method in https://arxiv.org/pdf/1902.07263.pdf\n",
    "\n",
    "Algorithm  \n",
    "\\begin{enumerate}\n",
    "\\item Calculate $g_{ij} = \\exp(-|X^i - X^j|^2/ 4\\epsilon)$ for $i,j = 1$ to $N$  \n",
    "\\item Calculate $k_{ij} = \\frac{g_{ij}}{\\sqrt{\\sum_l g_{il}}\\sqrt{\\sum_l g_{jl}}}$  \n",
    "\\item Calculate $d_i = \\sum_j k_{ij}$  \n",
    "\\item Calculate $\\text{T}_{ij} = \\frac{k_{ij}}{d_i}$  \n",
    "\\item Calculate $\\pi_i = \\frac{d_i}{\\sum_j d_j}$  \n",
    "\\item Calculate $ \\hat{h} = \\sum_{i = 1}^N \\pi_i h(X^i)$  \n",
    "\\item Until convergence, $\\Phi_i = \\sum_{j=1}^N \\text{T}_{ij} \\Phi_j + \\epsilon (h - \\hat{h})$  \n",
    "\\item Calculate $r_i = \\Phi_i + \\epsilon h_i$  \n",
    "\\item Calculate $s_{ij} = \\frac{1}{2\\epsilon} \\text{T}_{ij} (r_j - \\sum_{k=1}^N \\text{T}_{ik} r_k)$  \n",
    "\\item Calulate $\\text{K}_i  = \\sum_j s_{ij} X^j$\n",
    "\\end{enumerate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_coif(Xi, C, epsilon, Phi, No_iterations = parameters.coif_iterations, diag = 0):\n",
    "    start = timer()\n",
    "    \n",
    "    N,dim = Xi.shape\n",
    "    k = np.zeros((N,N))\n",
    "    K = np.zeros((N,dim))\n",
    "    d = np.zeros(N)\n",
    "    T = np.zeros((N,N))\n",
    "    Phi = np.zeros(N)\n",
    "    sum_term = np.zeros((N,dim))\n",
    "    max_diff = 1\n",
    "    \n",
    "    iterations = 1\n",
    "    \n",
    "    g = np.exp(- squareform(pdist(Xi,'euclidean'))**2/ (4 * epsilon))    \n",
    "    k = np.divide(g, np.sqrt((1/N) * np.sum(g,axis =0)))\n",
    "    k = np.divide(k.T, np.sqrt((1/N) * np.sum(g,axis =0)))    \n",
    "    d = np.sum(k, axis=0)\n",
    "    T = np.divide(k, np.sum(k,axis=0).reshape((-1,1)))   \n",
    "    \n",
    "    pi = np.divide(d, np.sum(d))\n",
    "    C_hat = np.dot(pi, C)\n",
    "                      \n",
    "    while((max_diff > parameters.coif_err_threshold) & ( iterations < No_iterations )):\n",
    "        Phi_new = np.dot(T,Phi) + (epsilon * np.concatenate(C - C_hat)).transpose() \n",
    "        max_diff = max(Phi_new - Phi) - min(Phi_new - Phi)\n",
    "        Phi  = Phi_new\n",
    "        iterations += 1\n",
    "    \n",
    "    r = Phi + epsilon * np.concatenate(C)\n",
    "    sum_term = np.dot(T, r)\n",
    "    for i in range(N):\n",
    "        K[i,:] = np.zeros(dim)\n",
    "        for j in range(N):\n",
    "            K[i,:] = K[i,:] + (1/ (2 * epsilon)) * T[i,j] * (r[j] - sum_term[i]) * Xi[j,:]                                  \n",
    "    if diag == 1:\n",
    "        plt.figure()\n",
    "        plt.plot(Xi, g[1,:], 'r*')\n",
    "        plt.show()\n",
    "    \n",
    "    end = timer()\n",
    "    print('Time taken for gain_coif()' , end - start)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_coif_prev(Xi, C, epsilon, Phi, No_iterations = 50000, diag = 0):\n",
    "    start = timer()\n",
    "    \n",
    "    N,dim = Xi.shape\n",
    "    k = np.zeros((N,N))\n",
    "    K = np.zeros((N,dim))\n",
    "    d = np.zeros(N)\n",
    "    T = np.zeros((N,N))\n",
    "    Phi = np.zeros(N)\n",
    "    sum_term = np.zeros((N,dim))\n",
    "    max_diff = 1\n",
    "    \n",
    "    iterations = 1\n",
    "    \n",
    "    g = np.exp(- squareform(pdist(Xi,'euclidean'))**2/ (4 * epsilon))    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            k[i,j] = g[i,j] / (np.sqrt( (1/N) * sum(g[i,:])) * np.sqrt( (1/N)* sum(g[j,:])))\n",
    "        d[i] = np.sum(k[i,:])\n",
    "        T[i,:] = np.divide(k[i,:], np.sum(k[i,:]))\n",
    "    pi = np.divide(d, np.sum(d))\n",
    "    C_hat = np.dot(pi, C)\n",
    "                      \n",
    "    while((max_diff > 0) & ( iterations < No_iterations )):\n",
    "        Phi_new = np.dot(T,Phi) + (epsilon * np.concatenate(C - C_hat)).transpose() \n",
    "        max_diff = max(Phi_new - Phi) - min(Phi_new - Phi)\n",
    "        Phi  = Phi_new\n",
    "        iterations += 1\n",
    "    \n",
    "    r = Phi + epsilon * np.concatenate(C)\n",
    "    for i in range(N):\n",
    "        sum_term[i] = np.dot( T[i,:], r)\n",
    "        K[i,:] = np.zeros(dim)\n",
    "        for j in range(N):\n",
    "            K[i,:] = K[i,:] + (1/ (2 * epsilon)) * T[i,j] * (r[j] - sum_term[i]) * Xi[j,:]                                  \n",
    "    if diag == 1:\n",
    "        plt.figure()\n",
    "        plt.plot(Xi, g[1,:], 'r*')\n",
    "        plt.show()\n",
    "    \n",
    "    end = timer()\n",
    "    print('Time taken for gain_coif()' , end - start)\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly older version of Markov kernel approximation - from https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7799105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_coif_old(Xi, C, epsilon, Phi, No_iterations =50000, diag = 0):\n",
    "    start = timer()\n",
    "    \n",
    "    N,dim = Xi.shape\n",
    "    k = np.zeros((N,N))\n",
    "    K = np.zeros((N,dim))\n",
    "    d = np.zeros(N)\n",
    "    T = np.zeros((N,N))\n",
    "    Phi = np.zeros(N)\n",
    "    sum_term = np.zeros((N,dim))\n",
    "    max_diff = 1\n",
    "        \n",
    "    iterations = 1\n",
    "    \n",
    "    g = np.exp(- squareform(pdist(Xi,'euclidean'))**2/ (4 * epsilon))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            k[i,j] = g[i,j] / (np.sqrt( (1/N) * sum(g[i,:])) * np.sqrt( (1/N)* sum(g[j,:])))\n",
    "        d[i] = np.sum(k[i,:])\n",
    "        T[i,:] = np.divide(k[i,:], np.sum(k[i,:]))\n",
    "                      \n",
    "    while((max_diff > 0) & ( iterations < No_iterations )):\n",
    "        Phi_new = np.dot(T,Phi) + (epsilon * np.concatenate(C)).transpose() \n",
    "        max_diff = max(Phi_new - Phi) - min(Phi_new - Phi)\n",
    "        Phi  = Phi_new\n",
    "        iterations += 1\n",
    "    \n",
    "    for i in range(N):\n",
    "        sum_term[i,:] = np.dot( T[i,:], Xi)\n",
    "        K[i,:] = np.zeros(dim)\n",
    "        for j in range(N):\n",
    "            K[i,:] = K[i,:] + (1/ (2 * epsilon)) * T[i,j] * Phi[j,] * (Xi[j,:] - sum_term[i,:])   \n",
    "            \n",
    "    if diag == 1:\n",
    "        plt.figure()\n",
    "        plt.plot(Xi, g[1,:], 'r*')\n",
    "        plt.show()\n",
    "    \n",
    "    end = timer()\n",
    "    print('Time taken for gain_coif_old()' , end - start)\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gain_rkhs_om() - Function to approximate FPF gain using RKHS OM method - Adds a Lagrangian parameter $\\mu$ to make use of the constant gain approximation\n",
    "Algorithm\n",
    "\n",
    "$\\beta^*$ obtained by solving the set of linear equations\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "0  &=  2 \\Bigl(  \\frac{1}{N}  \\sum_{k=1}^d M_{x_k}^T M_{x_k}   +  \\lambda M_0 \\Bigr) \\beta ^* + \\frac{ \\kappa \\mu ^*}{N}+  \\frac{2}{N} \\Bigl( \\kappa \\text{K}^*  -   M_0 \\tilde{c} \\Bigr)  \\\\\n",
    "0  & = \\kappa^{T} \\beta^*\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_rkhs_om(Xi, C, epsilon, Lambda, diag = 0):\n",
    "    start = timer()\n",
    "    \n",
    "    N,dim = Xi.shape\n",
    "    K = np.zeros((N,dim))\n",
    "    Ker_x = np.array(np.zeros((N,N,dim)))\n",
    "    # Ker_xy = np.array(np.zeros((N,N)))\n",
    "    \n",
    "    Ker = np.exp(- squareform(pdist(Xi,'euclidean'))**2/ (4 * epsilon))\n",
    "    \n",
    "    for i in range(N):\n",
    "        Ker_x[i,:,:] = np.multiply(-(Xi[i,:]-Xi) ,Ker[i,:].reshape((-1,1))) / (2 * epsilon)\n",
    "            # Ker_xy[i,j] = (((Xi[i] - Xi[j])**2) / (2 * epsilon) -1) * Ker[i,j] / (2 * epsilon)\n",
    "    Ker_x = Ker_x.astype(np.float32)\n",
    "    Ker_x_ones = np.dot(np.transpose(Ker_x), np.ones((N,1)))\n",
    "\n",
    "    eta = np.mean(C)\n",
    "    Y = (C -eta)\n",
    "    \n",
    "    K_hat = np.mean(Y * Xi, axis = 0)\n",
    "\n",
    "    b_m = (2/ N) * np.dot(Ker,Y) - (2/ N) * np.dot( np.moveaxis(Ker_x_ones,0,2), K_hat) \n",
    "    b_m = np.append(b_m, np.zeros((dim,1)))\n",
    "    \n",
    "    Ker_x_sum = np.array([np.dot(Ker_x[:,:,d_i], Ker_x[:,:,d_i].transpose()) for d_i in np.arange(dim)]).sum(axis =0)\n",
    "    M_m = 2 * Lambda * Ker + (2 / N) * Ker_x_sum\n",
    "    M_m = np.vstack((M_m, (1/N) * np.squeeze(Ker_x_ones)))\n",
    "    M_m = np.hstack((M_m, np.append(np.squeeze(np.transpose(Ker_x_ones),axis =0),np.zeros((dim,dim)),axis =0))) #.reshape(len(M_m),1))\n",
    "    \n",
    "    beta_m = np.linalg.solve(M_m,b_m)\n",
    "\n",
    "    K  = np.tile(K_hat, (N,1))\n",
    "    K  = K + np.dot(beta_m[:-dim].reshape((-1,1)).transpose(), Ker_x)\n",
    "    K  = np.squeeze(K)  \n",
    "    K  = K.reshape((K.shape[0],dim))   \n",
    "    \n",
    "    if diag == 1:\n",
    "        plt.figure()\n",
    "        plt.plot(Xi, Ker[:,0],'r*')\n",
    "        plt.plot(Xi, Ker_x[:,0], 'b*')\n",
    "        #plt.plot(Xi, Ker_xy[:,0],'k*')\n",
    "        plt.show()\n",
    "            \n",
    "    end = timer()\n",
    "    print('Time taken for gain_rkhs_om()' , end - start)\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter selection using grid search \n",
    "### select_hyperparameters() -  Hyper parameter selection for RKHS OM for dim $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def select_hyperparameters(method,Lambda = None,eps = None, No_runs = None, N = None, dim =None): \n",
    "    # Choose hyperparameters for which algorithm\n",
    "    if method is None:\n",
    "        method = input('Input the algorithm (coif, rkhs_N, rkhs_dN, om,) ')\n",
    "    \n",
    "    # Run parameters\n",
    "    if No_runs is None:\n",
    "        No_runs = input('Input the number of independent runs - ')  \n",
    "    if No_runs =='':\n",
    "        No_runs = 100\n",
    "    else:\n",
    "        No_runs = int(No_runs)\n",
    "        \n",
    "    # FPF parameters - No. of particles\n",
    "    if N is None:\n",
    "        N = input('Input the number of particles - ')\n",
    "    if N =='':\n",
    "        N = 500\n",
    "    else:\n",
    "        N = int(N)\n",
    "        \n",
    "    # System parameters\n",
    "    if dim is None:\n",
    "        dim = input('Input the dimension of the system - ') # dimension of the system\n",
    "    if dim =='':\n",
    "        dim = 1\n",
    "    else:\n",
    "        dim = int(dim)\n",
    "        \n",
    "    x = symbols('x0:%d'%dim)\n",
    "    c_coef = np.ones((1,dim)) \n",
    "    c =  c_coef.dot(x)      # Observation function (Eg: for d = 2, c(x) = x1 + x2)\n",
    "    c_x = lambdify(x, c, 'numpy')\n",
    "\n",
    "        \n",
    "    # Parameters of the prior density \\rho_B - 2 component Gaussian mixture density\n",
    "    gm = dim     # No. of dimensions with Gaussian mixture densities in the dim-dimensional density, should be <= dim\n",
    "    p_b = 0\n",
    "    for m in range(len(parameters.w_b)):\n",
    "        p_b = p_b + parameters.w_b[m] * (1/ np.sqrt(2 * math.pi * parameters.sigma_b[m]**2))* exp(-(x[0] - parameters.mu_b[m])**2/ (2* parameters.sigma_b[m]**2))\n",
    "    # Standard deviation for the Gaussian component (if any)    \n",
    "    # Hyperparameter grid\n",
    "    if eps is None:\n",
    "        eps = parameters.eps\n",
    "    if Lambda is None:\n",
    "        Lambda = parameters.Lambda\n",
    "        \n",
    "    if method == 'coif':\n",
    "        mse  = np.zeros((No_runs, len(eps)))\n",
    "    else:\n",
    "        mse  = np.zeros((No_runs, len(eps), len(Lambda)))\n",
    "    mse_const= np.zeros(No_runs)  # Used as baseline to compare the performance of the method\n",
    "    \n",
    "    print('Setup')\n",
    "    print('No. of independent runs ', No_runs)\n",
    "    print('Dimensions ', dim)\n",
    "    print('===============')\n",
    "    for run in range(No_runs):\n",
    "        clear_output()\n",
    "        print('No. of particles ', N)\n",
    "        print('Dimensions ', dim)\n",
    "        print('Run ',run+1 ,' of ', No_runs)\n",
    "        # Xi  = get_samples(N, mu_b, sigma_b, w_b, dim, gm, sigma, seed = run)\n",
    "        Xi  = get_samples(N, parameters.mu_b, parameters.sigma_b, parameters.w_b, dim, gm, parameters.sigma)\n",
    "        if dim == 1:\n",
    "            Xi = np.sort(Xi,kind = 'mergesort')\n",
    "        C = np.reshape(c_x(*Xi.T),(len(Xi),1))\n",
    "    \n",
    "        K_exact = np.zeros((N, dim))\n",
    "        for d in range(gm):\n",
    "            K_exact[:,d]  = gain_num_integrate(Xi, x[0], p_b, x, d)\n",
    "        \n",
    "        for i,eps_i in enumerate(eps):\n",
    "            if method == 'coif':\n",
    "                Phi = np.zeros(N)\n",
    "                K_approx = gain_coif(Xi, C, eps_i, Phi, diag = 0) \n",
    "                mse[run, i] = mean_squared_error(K_exact, K_approx)\n",
    "            elif method == 'rkhs_N':\n",
    "                for j,Lambda_j in enumerate(Lambda):  \n",
    "                    K_approx = gain_rkhs_N(Xi, C, eps_i, Lambda_j, diag = 0)\n",
    "                    mse[run, i,j] = mean_squared_error(K_exact, K_approx)\n",
    "            elif method == 'rkhs_dN':\n",
    "                for j,Lambda_j in enumerate(Lambda):  \n",
    "                    K_approx = gain_rkhs_dN(Xi, C, eps_i, Lambda_j, diag = 0)\n",
    "                    mse[run, i,j] = mean_squared_error(K_exact, K_approx)\n",
    "            elif method == 'om':\n",
    "                for j,Lambda_j in enumerate(Lambda):  \n",
    "                    K_approx = gain_rkhs_om(Xi, C, eps_i, Lambda_j, diag = 0)\n",
    "                    mse[run, i,j] = mean_squared_error(K_exact, K_approx)\n",
    "            else:\n",
    "                for j,Lambda_j in enumerate(Lambda):  \n",
    "                    print('Invalid method provided')\n",
    "        \n",
    "        # Baseline error calculation        \n",
    "        eta = np.mean(C)\n",
    "        Y = (C -eta)\n",
    "        K_const = np.mean(Y * Xi, axis = 0)\n",
    "        mse_const[run] = mean_squared_error(K_exact, K_const)     \n",
    "        \n",
    "    if method == 'coif':\n",
    "        i_min = np.argmin(np.mean(mse,axis=0))\n",
    "        print('Best value of $\\epsilon', eps[i_min])\n",
    "        return mse, eps, eps[i_min]\n",
    "    else:\n",
    "        i_min, j_min = np.unravel_index(np.argmin(np.mean(mse,axis =0)),np.mean(mse, axis =0).shape)\n",
    "        print('Best value of  $\\lambda$', Lambda[j_min])\n",
    "        print('Best value of $\\epsilon$', eps[i_min])\n",
    "        return mse,Lambda, eps, Lambda[j_min], eps[i_min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### contour_lambda_eps() - Function to plot contour plots of mses vs a grid of $\\lambda$ and $\\epsilon$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def contour_lambda_eps(mse_mean, Lambda, eps, contour_levels = None):\n",
    "    fig = plt.figure(figsize =(10,8))\n",
    "    if contour_levels:\n",
    "        cont = plt.contourf(eps, np.log10(Lambda),mse_mean.transpose(), contour_levels)\n",
    "    else:\n",
    "        cont = plt.contourf(eps, np.log10(Lambda),mse_mean.transpose())\n",
    "\n",
    "    # cont = plt.contourf(eps, np.log10(Lambda),mse_mean.transpose())\n",
    "    fig.colorbar(cont)\n",
    "    plt.xlabel('$\\epsilon$')\n",
    "    plt.xticks(fontsize = 24)\n",
    "    plt.yticks(fontsize = 24)\n",
    "    plt.ylabel('$\\log_{10}(\\lambda)$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_hist_mse() - Function to plot a histogram of mses obtained from independent trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_mse(mse,Lambda,eps):\n",
    "    plt.figure(figsize = (10,8))    \n",
    "    for i,eps_i in enumerate(eps):\n",
    "            for j,Lambda_j in enumerate(Lambda):\n",
    "                sns.distplot(mse[:,i,j], label = str(Lambda_j) +',' +str(eps_i))\n",
    "                plt.legend()\n",
    "    plt.title('Histograms of mse obtained using various algorithms for '+ str(No_runs) + ' trials')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
